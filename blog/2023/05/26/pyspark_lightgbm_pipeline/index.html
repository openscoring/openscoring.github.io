<!DOCTYPE html>
<html lang="en">

<head>
  <script src="https://www.googletagmanager.com/gtag/js?id=G-1XC1KCZX17" async></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'G-1XC1KCZX17');
</script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
  <meta name="format-detection" content="telephone=no">
  <meta name="theme-color" content="#ffffff">
  <meta name="msapplication-navbutton-color" content="#ffffff">
  <meta name="apple-mobile-web-app-status-bar-style" content="#ffffff">

  <meta name="description" content="">
  <meta name="keywords" content="apache-spark pyspark synapseml data-categorical data-invalid data-missing">
  <meta name="author" content="vruusmann">

  <title>Training PySpark LightGBM pipelines - Openscoring</title>

  <meta name="robots" content="index, follow, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
  <link rel="canonical" href="https://openscoring.io/blog/2023/05/26/pyspark_lightgbm_pipeline/">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Training PySpark LightGBM pipelines - Openscoring">
  <meta property="og:url" content="/blog/2023/05/26/pyspark_lightgbm_pipeline/">
  <meta property="og:site_name" content="Openscoring">
  <meta property="og:updated_time" content="2023-05-26 00:00:00 +0300">
  <meta property="article:published_time" content="2023-05-26 00:00:00 +0300">
  <meta property="article:modified_time" content="2023-05-26 00:00:00 +0300">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Training PySpark LightGBM pipelines - Openscoring">
  <meta name="twitter:label1" content="Written by">
  <meta name="twitter:data1" content="Villu Ruusmann">
  <meta name="twitter:label2" content="Time to read">
  <meta name="twitter:data2" content="Less than a minute">

  <link rel="icon" href="/assets/images/fa-150x150.png" sizes="32x32">
  <link rel="icon" href="/assets/images/fa.png" sizes="192x192">
  <link rel="apple-touch-icon" href="/assets/images/fa.png">
  <link rel="stylesheet" href="/assets/css/main.css">
  <meta name="msapplication-TileImage" content="h/assets/images/fa.png">
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<body>
  <header class="header">
  <div class="container">
    <nav class="nav--main">
      <a class="logo" href="/" aria-label="home">
        <img src="/assets/images/logo.svg" alt="" width="35" height="32" loading="lazy">
        <span>Openscoring</span>
      </a>

      <input type="checkbox" id="nav__toggle--main" class="nav__toggle">
      <label for="nav__toggle--main">Menu<span></span></label>

      <div class="menu">
        <ul id="menu--main">
          <li><a href="/#overview" aria-current="page">Overview</a></li>
          <li><a href="/#products" aria-current="page">Products</a></li>
          <li><a href="/#licensing" aria-current="page">Licensing</a></li>
          <li><a href="/#consulting" aria-current="page">Consulting</a></li>
        </ul>
      </div>

      <label for="nav__toggle--main" class="overlay"></label>

      <a href="/blog/" class="btn btn--small">Blog</a>
    </nav>
  </div>
</header>

  <main class="container mt-26">
    <h1>Training PySpark LightGBM pipelines</h1>
  
<div class="post">
  <p>LightGBM supports “distributed learning” mode, where the training of a single model is split between multiple computers.
Apache Spark users have the best-in-kind access to it using the <a href="https://github.com/microsoft/SynapseML">SynapseML</a> (formerly MMLSpark) middleware library.</p>

<p>However, pushing LightGBM to its fullest potential in custom environments remains challenging.
This blog post demonstrates how to build PySpark pipelines for complex real-life datasets so that their key aspects (categorical features, missing values) are correctly presented.</p>

<h2 id="setup">Setup</h2>

<p>The PySpark LightGBM software stack has three major components:</p>
<ol>
  <li>LightGBM C++ library.</li>
  <li>SynapseML Java wrapper library, which provides Apache Spark API for commanding #1.</li>
  <li>SynapseML Python wrapper library, which provides PySpark API for commanding #2.</li>
</ol>

<p>The installation is split into two units.
Namely, components #1 and #2 are packaged and distributed together as a Apache Spark package, whereas component #3 is a standalone Python package.</p>

<h3 id="apache-spark-side">Apache Spark side</h3>

<p>Installing and activating the <code class="language-plaintext highlighter-rouge">com.microsoft.azure:synapseml-lightgbm_2.12</code> package:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ $SPARK_HOME/bin/spark-submit --packages "com.microsoft.azure:synapseml-lightgbm_2.12:${synapseml.version}" main.py
</code></pre></div></div>

<p>At the time of writing this (May 2023), the Maven Central repository contains <a href="https://search.maven.org/artifact/com.microsoft.azure/synapseml-lightgbm_2.12">six versions of this artifact</a>:</p>

<table>
  <thead>
    <tr>
      <th>SynapseML version</th>
      <th>LightGBM version</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.9.5</td>
      <td>3.2.1</td>
    </tr>
    <tr>
      <td>0.10.0</td>
      <td>3.2.1</td>
    </tr>
    <tr>
      <td>0.10.1</td>
      <td>3.2.1</td>
    </tr>
    <tr>
      <td>0.10.2</td>
      <td>3.2.1</td>
    </tr>
    <tr>
      <td>0.11.0</td>
      <td>3.3.3</td>
    </tr>
    <tr>
      <td>0.11.1</td>
      <td>3.3.5</td>
    </tr>
  </tbody>
</table>

<p>It can be seen that SynapseML generations 0.9 and 0.10 depend on LightGBM version 3.2.1, whereas SynapseML generation 0.11 depends on newer LightGBM versions 3.3.3 and 3.3.5.
However, from the data science perspective, they all should be more or less functionally equivalent.</p>

<h3 id="python-side">Python side</h3>

<p>Installing the <code class="language-plaintext highlighter-rouge">synapseml</code> package:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python -m pip install synapseml==${synapseml.version}
</code></pre></div></div>

<p>The safest option would be to use identical SynapseML package versions on both sides.</p>

<p>Checking the installation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyspark</span>

<span class="k">print</span><span class="p">(</span><span class="s">"PySpark version: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pyspark</span><span class="p">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
  <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Spark version: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">version</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">synapse.ml.lightgbm</span> <span class="k">as</span> <span class="n">sml_lightgbm</span>

<span class="k">print</span><span class="p">(</span><span class="s">"SynapseML version: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sml_lightgbm</span><span class="p">.</span><span class="n">__version__</span><span class="p">))</span>
</code></pre></div></div>

<p>The SynapseML Python wrapper library can be proven by loading the <code class="language-plaintext highlighter-rouge">synapse.ml.lightgbm</code> module, and querying its <code class="language-plaintext highlighter-rouge">__version__</code> attribute.
However, this does not convey any information about the underlying library layers.</p>

<p>Next, the SynapseML Java wrapper library can be proven by attempting to use some Java-backed functionality, such as constructing a dummy estimator:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">synapse.ml.lightgbm</span> <span class="kn">import</span> <span class="n">LightGBMClassifier</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LightGBMClassifier</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">classifier</span><span class="p">)</span>
</code></pre></div></div>

<p>If the Apache Spark package is not active, then this will fail with a characteristic Py4J type error <code class="language-plaintext highlighter-rouge">TypeError: 'JavaPackage' object is not callable</code>.</p>

<p>The version of the SynapseML Java wrapper library in use is not directly queriable.
With newer PySpark versions, it can be deduced by <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.listFiles.html#pyspark.SparkContext.listFiles">listing Apache Spark resource files</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span>

<span class="c1"># Requires PySpark 3.4 or newer
</span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="s">"listFiles"</span><span class="p">):</span>
  <span class="n">synapsemlResourceFiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">scFile</span> <span class="k">for</span> <span class="n">scFile</span> <span class="ow">in</span> <span class="n">sc</span><span class="p">.</span><span class="n">listFiles</span> <span class="k">if</span> <span class="s">"synapseml"</span> <span class="ow">in</span> <span class="n">scFile</span><span class="p">]</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Spark SynapseML resource files: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">synapsemlResourceFiles</span><span class="p">))</span>
</code></pre></div></div>

<p>Alternatively, it can be noted from the Apache Spark log as the <code class="language-plaintext highlighter-rouge">SynapseMLLogInfo.buildVersion</code> entry:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>23/05/26 10:25:23 INFO LightGBMClassifier: metrics/ {"buildVersion":"0.10.2","className":"class com.microsoft.azure.synapse.ml.lightgbm.LightGBMClassifier","method":"constructor","uid":"LightGBMClassifier_903c5f1b3b2e"}
</code></pre></div></div>

<p>Finally, the LightGBM C++ library could be proven by fitting the dummy estimator.
However, given the tight physical and logical coupling between components #1 and #2, this check is likely to succeed at all times.</p>

<h2 id="training">Training</h2>

<p>Pipeline template:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">StringIndexer</span><span class="p">,</span> <span class="n">VectorAssembler</span>
<span class="kn">from</span> <span class="nn">synapse.ml.lightgbm</span> <span class="kn">import</span> <span class="n">LightGBMClassifier</span>

<span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[...]</span>
<span class="n">cont_cols</span> <span class="o">=</span> <span class="p">[...]</span>

<span class="n">labelIndexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(...)</span>
<span class="n">labelIndexerModel</span> <span class="o">=</span> <span class="n">labelIndexer</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">catColumnsIndexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(</span><span class="n">inputCols</span> <span class="o">=</span> <span class="n">cat_cols</span><span class="p">,</span> <span class="n">outputCols</span> <span class="o">=</span> <span class="p">[</span><span class="s">"idx"</span> <span class="o">+</span> <span class="n">cat_col</span> <span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">cat_cols</span><span class="p">])</span>

<span class="n">vectorAssembler</span> <span class="o">=</span> <span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span> <span class="o">=</span> <span class="n">catColumnsIndexer</span><span class="p">.</span><span class="n">getOutputCols</span><span class="p">()</span> <span class="o">+</span> <span class="n">cont_cols</span><span class="p">,</span> <span class="n">outputCol</span> <span class="o">=</span> <span class="s">"featureVector"</span><span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LightGBMClassifier</span><span class="p">(</span><span class="n">objective</span> <span class="o">=</span> <span class="s">"binary"</span><span class="p">,</span> <span class="n">numIterations</span> <span class="o">=</span> <span class="mi">117</span><span class="p">,</span> <span class="n">labelCol</span> <span class="o">=</span> <span class="n">labelIndexerModel</span><span class="p">.</span><span class="n">getOutputCol</span><span class="p">(),</span> <span class="n">featuresCol</span> <span class="o">=</span> <span class="n">vectorAssembler</span><span class="p">.</span><span class="n">getOutputCol</span><span class="p">())</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">stages</span> <span class="o">=</span> <span class="p">[</span><span class="n">labelIndexerModel</span><span class="p">,</span> <span class="n">catColumnsIndexer</span><span class="p">,</span> <span class="n">vectorAssembler</span><span class="p">,</span> <span class="n">classifier</span><span class="p">])</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="categorical-data">Categorical data</h3>

<p>LightGBM can do both continuous and categorical splits.</p>

<p>Categorical splits are attempted on integer columns that have been tagged as categorical.
The simplest way to perform such data pre-processing is using the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html"><code class="language-plaintext highlighter-rouge">StringIndexer</code></a> transformer.</p>

<p>The tagging is based on column metadata.
Specifically, <code class="language-plaintext highlighter-rouge">StringIndexer</code> output columns carry a <code class="language-plaintext highlighter-rouge">ml_attr</code> tag, which signals that these double values represent array indices rather than generic data.
The SynapseML Java wrapper library must be relying on this information when auto-detecting the list of categorical features.</p>

<p>The auto-detection algorithm is disabled when this list was already declared during LightGBM model construction.</p>

<p>There are two declaration approaches possible.
First, in the name-based approach, the data scientist assigns names to all feature vector elements using the <code class="language-plaintext highlighter-rouge">slotNames</code> parameter, and then identifies the categorical subset using the <code class="language-plaintext highlighter-rouge">categoricalSlotNames</code> parameter:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span> <span class="o">=</span> <span class="n">LightGBMClassifier</span><span class="p">(</span><span class="n">slotNames</span> <span class="o">=</span> <span class="n">cat_cols</span> <span class="o">+</span> <span class="n">cont_cols</span><span class="p">,</span> <span class="n">categoricalSlotNames</span> <span class="o">=</span> <span class="n">cat_cols</span><span class="p">,</span> <span class="p">...)</span>
</code></pre></div></div>

<p>Second, in the index-based approach, the data scientist identifies the indices of categorical feature vector elements directly using the <code class="language-plaintext highlighter-rouge">categoricalSlotIndexes</code> parameter:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cat_col_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_cols</span><span class="p">)))</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LightGBMClassifier</span><span class="p">(</span><span class="n">categoricalSlotIndexes</span> <span class="o">=</span> <span class="n">cat_col_indices</span><span class="p">,</span> <span class="p">...)</span>
</code></pre></div></div>

<p>After fitting a LightGBM model, it is advisable to inspect its internal structure to see if all categorical features were detected and handled as such.
Getting the operational type of a feature wrong may do serious damage to its interpretability and predictive performance.</p>

<p>People who are more familiar with LightGBM internals can export the booster object into a text file, and inspect its header section for feature definitions as expressed in terms of <code class="language-plaintext highlighter-rouge">feature_names</code> and <code class="language-plaintext highlighter-rouge">feature_infos</code> entries.
Categorical feature infos match the <code class="language-plaintext highlighter-rouge">-1:&lt;value_1&gt;:&lt;value_2&gt;:..:&lt;value_n&gt;</code> pattern (ie. colon-separated enumeration of possible integer values), whereas continuous feature infos match the <code class="language-plaintext highlighter-rouge">[&lt;value_min&gt;:&lt;value_max&gt;]</code> pattern (ie. colon-separated range bounds).</p>

<p>The SynapseML exported booster text file does not include any helper sections that would elaborate the mapping of category levels between <code class="language-plaintext highlighter-rouge">StringIndexer</code> input and output columns.
This deficiency makes the migration of booster objects between application environments difficult.</p>

<p>If the booster object needs to be shared with Scikit-Learn, then the workaround will be to generate and append a <code class="language-plaintext highlighter-rouge">pandas_categorical</code>-style helper section to the booster text file.</p>

<h3 id="sparse-data">Sparse data</h3>

<p>LightGBM generates binary splits, and tags one of the two branches as the “default branch”.
If a data record contains fields with missing values, then they are not evaluated against the actual split condition, but are directly assigned to the default branch.
The same treatment applies to invalid values.</p>

<p>Technically speaking, there are three kinds of value spaces possible:</p>
<ul>
  <li>Valid. A non-missing value, which was seen in the training dataset.</li>
  <li>Invalid. A non-missing value in a validation or testing dataset, which was not seen in the training dataset.</li>
  <li>Missing.</li>
</ul>

<p>To rehash, a model is fitted on a dataset that contains valid and, possibly, missing values.
However, it can be used for prediction on datasets that contain all valid, missing and invalid values.</p>

<p>Popular ML frameworks such as Scikit-Learn and Apache Spark enforce a naivist approach, where data pre-processing transformers must perform data validation so that incoming non-valid values (ie. missing and invalid values) are transformed to valid values.</p>

<p>For example, in PySpark pipelines, missing values are stopped already at the forefront by replacing them with constant values (eg. sample mean, median or mode) using the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html"><code class="language-plaintext highlighter-rouge">Imputer</code></a> transformer.
All the subsequent transformers act under the assumption that they will never encounter a missing value.</p>

<p>Adding support for missing and invalid values is all about getting Apache Spark’s built-in data validation logic out of the way.
All values should be allowed to pass from one stage to another, up until the final model stage.</p>

<p>The fix is centered around setting the <code class="language-plaintext highlighter-rouge">handleInvalid</code> attribute of transformers from “error” to “keep”:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vectorAssembler</span> <span class="o">=</span> <span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span> <span class="o">=</span> <span class="n">catColumnsIndexer</span><span class="p">.</span><span class="n">getOutputCols</span><span class="p">()</span> <span class="o">+</span> <span class="n">cont_cols</span><span class="p">,</span> <span class="n">outputCol</span> <span class="o">=</span> <span class="s">"featureVector"</span><span class="p">,</span> <span class="n">handleInvalid</span> <span class="o">=</span> <span class="s">"keep"</span><span class="p">)</span>
</code></pre></div></div>

<p>The “keep” treatment is very effective with transformers that deal with numeric values.
However, the same cannot be said about various Apache Spark encoders and discretizers that deal with non-numeric values, because their transformation behaviour is more intrusive than it needs to be.</p>

<p>The case in point is once again the <code class="language-plaintext highlighter-rouge">StringIndexer</code> transformer.
It is expected that the “keep” treatment should transform <code class="language-plaintext highlighter-rouge">None</code> values to <code class="language-plaintext highlighter-rouge">NaN</code> values (string columns), or let <code class="language-plaintext highlighter-rouge">NaN</code> values pass through as-is (double columns).
In reality, the <code class="language-plaintext highlighter-rouge">StringIndexer</code> transformer maps all non-valid values to a special additional bucket, at index <code class="language-plaintext highlighter-rouge">len(labels)</code>.</p>

<p>The desired transformation behaviour would be to emit a <code class="language-plaintext highlighter-rouge">NaN</code> value or some negative integer value (ie. something that is easily distinguishable from “legible” array indices).
Unfortunately, the <code class="language-plaintext highlighter-rouge">StringIndexer</code> transformer cannot be configured to behave this way.</p>

<p>The workaround is to move the encoding of categorical features completely out of the main pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">catColumnsIndexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(</span><span class="n">inputCols</span> <span class="o">=</span> <span class="n">cat_cols</span><span class="p">,</span> <span class="n">outputCols</span> <span class="o">=</span> <span class="p">[</span><span class="s">"idx"</span> <span class="o">+</span> <span class="n">cat_col</span> <span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">cat_cols</span><span class="p">],</span> <span class="n">handleInvalid</span> <span class="o">=</span> <span class="s">"keep"</span><span class="p">)</span>
<span class="n">catColumnsIndexerModel</span> <span class="o">=</span> <span class="n">catColumnsIndexer</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">catColumnsIndexerModel</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Replace the maximum value for each output column with -999 
</span><span class="k">for</span> <span class="n">outputCol</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">catColumnsIndexerModel</span><span class="p">.</span><span class="n">getOutputCols</span><span class="p">(),</span> <span class="n">catColumnsIndexerModel</span><span class="p">.</span><span class="n">labelsArray</span><span class="p">):</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)),</span> <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="mi">999</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="n">outputCol</span><span class="p">])</span>
</code></pre></div></div>

<p>LightGBM will canonicalize all replacement values to <code class="language-plaintext highlighter-rouge">NaN</code> values, as made evident by Apache Spark log messages <code class="language-plaintext highlighter-rouge">[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN</code>.
There is exactly one such log message being printed per column.</p>

<p>The <code class="language-plaintext highlighter-rouge">DataFrame.replace()</code> method appears to re-create columns (rather than updating values in place), because all <code class="language-plaintext highlighter-rouge">ml_attr</code> tags are lost.
This means that the list of categorical features cannot be auto-detected under no circumstances.
It must be declared during LightGBM model construction, using either the name-based or index-based approach (see the “categorical data” section above).</p>

<p>The training succeeds with all Apache Spark and SynapseML version combinations.</p>

<h2 id="persistence">Persistence</h2>

<p>Fitted <code class="language-plaintext highlighter-rouge">PipelineModel</code> objects can be persisted for later deployment(s) using the <code class="language-plaintext highlighter-rouge">save()</code> method.</p>

<p>Unfortunately, Apache Spark versions 3.0.X and 3.1.X do not support the saving of embedded LightGBM models due to the following Py4J Java error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "train.py", line 28, in &lt;module&gt;
    pipelineModel.save("LightGBMAudit")
  File "/opt/spark-3.0.3/python/lib/pyspark.zip/pyspark/ml/util.py", line 175, in save
  File "/opt/spark-3.0.3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/opt/spark-3.0.3/python/lib/pyspark.zip/pyspark/sql/utils.py", line 128, in deco
  File "/opt/spark-3.0.3/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o533.save.
: java.lang.NoClassDefFoundError: org/json4s/JsonListAssoc$
    at org.apache.spark.ml.ComplexParamsWriter$.getMetadataToSave(ComplexParamsSerializer.scala:126)
    at org.apache.spark.ml.ComplexParamsWriter$.saveMetadata(ComplexParamsSerializer.scala:97)
    at org.apache.spark.ml.ComplexParamsWriter.saveImpl(ComplexParamsSerializer.scala:40)
    at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)
    ...
Caused by: java.lang.ClassNotFoundException: org.json4s.JsonListAssoc$
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 43 more
</code></pre></div></div>

<p>The SynapseML Java wrapper library has been compiled against a <a href="https://github.com/json4s/json4s">JSON4S</a> library version that is newer than the one(s) that is bundled with the Apache Spark installation.</p>

<p>This classpath issue cannot be resolved by end users.</p>

<p>The workaround is to extract the LightGBM model from the pipeline, and save it separately as a booster text file:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">PipelineModel</span>

<span class="c1"># The Apache Spark part - all stages except for the final model stage
</span><span class="n">preprocPipelineModel</span> <span class="o">=</span> <span class="n">PipelineModel</span><span class="p">(</span><span class="n">pipelineModel</span><span class="p">.</span><span class="n">stages</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">preprocPipelineModel</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"PipelineModel"</span><span class="p">)</span>

<span class="c1"># The SynapseML part - the final model stage
</span><span class="n">lgbmModel</span> <span class="o">=</span> <span class="n">pipelineModel</span><span class="p">.</span><span class="n">stages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">lgbmModel</span><span class="p">.</span><span class="n">saveNativeModel</span><span class="p">(</span><span class="s">"LightGBMClassificationModel"</span><span class="p">)</span>
</code></pre></div></div>

<p>Loading:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">synapse.ml.lightgbm</span> <span class="kn">import</span> <span class="n">LightGBMClassificationModel</span>

<span class="n">preprocPipelineModel</span> <span class="o">=</span> <span class="n">PipelineModel</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"PipelineModel"</span><span class="p">)</span>

<span class="n">lgbmModel</span> <span class="o">=</span> <span class="n">LightGBMClassificationModel</span><span class="p">.</span><span class="n">loadNativeModelFromFile</span><span class="p">(</span><span class="s">"LightGBMClassificationModel"</span><span class="p">)</span>

<span class="c1"># Restore optional attributes
</span><span class="n">lgbmModel</span><span class="p">.</span><span class="n">setLabelCol</span><span class="p">(</span><span class="n">labelIndexerModel</span><span class="p">.</span><span class="n">getOutputCol</span><span class="p">())</span>
<span class="n">lgbmModel</span><span class="p">.</span><span class="n">setFeaturesCol</span><span class="p">(</span><span class="n">vectorAssembler</span><span class="p">.</span><span class="n">getOutputCol</span><span class="p">())</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">PipelineModel</span><span class="p">(</span><span class="n">stages</span> <span class="o">=</span> <span class="n">preprocPipelineModel</span><span class="p">.</span><span class="n">stages</span> <span class="o">+</span> <span class="p">[</span><span class="n">lgbmModel</span><span class="p">])</span>
</code></pre></div></div>

<p>The original pipeline and its saved-and-loaded clone make identical predictions.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li>Datasets: <a href="https://openscoring.io/resources/data/audit.csv"><code class="language-plaintext highlighter-rouge">audit.csv</code></a> and <a href="https://openscoring.io/resources/data/audit-NA.csv"><code class="language-plaintext highlighter-rouge">audit-NA.csv</code></a></li>
  <li>Python scripts: <a href="https://openscoring.io/resources/2023-05-26/check.py"><code class="language-plaintext highlighter-rouge">check.py</code></a>, <a href="https://openscoring.io/resources/2023-05-26/train.py"><code class="language-plaintext highlighter-rouge">train.py</code></a> and <a href="https://openscoring.io/resources/2023-05-26/train-NA.py"><code class="language-plaintext highlighter-rouge">train-NA.py</code></a></li>
</ul>


<h2>Feedback</h2>

<script src="https://giscus.app/client.js"
  data-repo="vruusmann/openscoring.io"
  data-repo-id="R_kgDOH23Yfg"
  data-category="Blog"
  data-category-id="DIC_kwDOH23Yfs4CaTiG"
  data-mapping="url"
  data-strict="0"
  data-reactions-enabled="0"
  data-emit-metadata="0"
  data-input-position="bottom"
  data-theme="light"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>

</div>

  </main>

  <footer class="footer">
  <div class="container">
    <div class="mb-6">© 2022 - Openscoring</div>

    <div class="mb-6">
      <ul class="flex gap-4 list-none">
        <li>
          <a href="mailto:info@openscoring.io" aria-label="email">
            <img src="/assets/images/email_round.svg" width="48" height="48" alt="Contact Openscoring">
          </a>
        </li>
        <li>
          <a href="https://twitter.com/openscoring" target="_blank" aria-label="twitter">
            <img src="/assets/images/twitter_round.svg" width="48" height="48" alt="Follow Openscoring on Twitter">
          </a>
        </li>
      </ul>
    </div>
  </div>
</footer>

  <script>
  var sc_project=11704106;
  var sc_security="a7d1bf16"; 
  var sc_invisible=1; 
  var sc_remove_link=1; 
</script>

<script src="https://www.statcounter.com/counter/counter.js" async></script>

<noscript>
  <div class="statcounter">
    <img class="statcounter" src="https://c.statcounter.com/11704106/0/a7d1bf16/1/" alt="Web Analytics Made Easy - Statcounter" referrerPolicy="no-referrer-when-downgrade">
  </div>
</noscript>
</body>

</html>
