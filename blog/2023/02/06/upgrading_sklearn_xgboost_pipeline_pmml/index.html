<!DOCTYPE html>
<html lang="en">

<head>
  <script src="https://www.googletagmanager.com/gtag/js?id=G-1XC1KCZX17" async></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'G-1XC1KCZX17');
</script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
  <meta name="format-detection" content="telephone=no">
  <meta name="theme-color" content="#ffffff">
  <meta name="msapplication-navbutton-color" content="#ffffff">
  <meta name="apple-mobile-web-app-status-bar-style" content="#ffffff">

  <meta name="description" content="">
  <meta name="keywords" content="scikit-learn xgboost sklearn2pmml data-categorical">
  <meta name="author" content="vruusmann">

  <title>Upgrading Scikit-Learn XGBoost pipelines - Openscoring</title>

  <meta name="robots" content="index, follow, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
  <link rel="canonical" href="https://openscoring.io/blog/2023/02/06/upgrading_sklearn_xgboost_pipeline_pmml/">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Upgrading Scikit-Learn XGBoost pipelines - Openscoring">
  <meta property="og:url" content="/blog/2023/02/06/upgrading_sklearn_xgboost_pipeline_pmml/">
  <meta property="og:site_name" content="Openscoring">
  <meta property="og:updated_time" content="2023-02-06 00:00:00 +0200">
  <meta property="article:published_time" content="2023-02-06 00:00:00 +0200">
  <meta property="article:modified_time" content="2023-02-06 00:00:00 +0200">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Upgrading Scikit-Learn XGBoost pipelines - Openscoring">
  <meta name="twitter:label1" content="Written by">
  <meta name="twitter:data1" content="Villu Ruusmann">
  <meta name="twitter:label2" content="Time to read">
  <meta name="twitter:data2" content="Less than a minute">

  <link rel="icon" href="/assets/images/fa-150x150.png" sizes="32x32">
  <link rel="icon" href="/assets/images/fa.png" sizes="192x192">
  <link rel="apple-touch-icon" href="/assets/images/fa.png">
  <link rel="stylesheet" href="/assets/css/main.css">
  <meta name="msapplication-TileImage" content="h/assets/images/fa.png">
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<body>
  <header class="header">
  <div class="container">
    <nav class="nav--main">
      <a class="logo" href="/" aria-label="home">
        <img src="/assets/images/logo.svg" alt="" width="35" height="32" loading="lazy">
        <span>Openscoring</span>
      </a>

      <input type="checkbox" id="nav__toggle--main" class="nav__toggle">
      <label for="nav__toggle--main">Menu<span></span></label>

      <div class="menu">
        <ul id="menu--main">
          <li><a href="/#overview" aria-current="page">Overview</a></li>
          <li><a href="/#products" aria-current="page">Products</a></li>
          <li><a href="/#licensing" aria-current="page">Licensing</a></li>
          <li><a href="/#consulting" aria-current="page">Consulting</a></li>
        </ul>
      </div>

      <label for="nav__toggle--main" class="overlay"></label>

      <a href="/blog/" class="btn btn--small">Blog</a>
    </nav>
  </div>
</header>

  <main class="container mt-26">
    <h1>Upgrading Scikit-Learn XGBoost pipelines</h1>
  
<div class="post">
  <p>XGBoost is one of the top algorithms for solving Tabular ML problems.</p>

<p>The <a href="https://github.com/dmlc/xgboost/tree/master/python-package"><code class="language-plaintext highlighter-rouge">xgboost</code></a> package provides XGBoost functionality in two flavours.
First, the low-level <a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training">Python Learning API</a> aims at API parity with the underlying C(++) library. Second, the high-level <a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn">Scikit-Learn API</a> aims at making the most popular parts accessible via Scikit-Learn style wrappers.</p>

<p>According to the <a href="https://pypi.org/project/xgboost/#history">XGBoost PyPI release history</a>, the <code class="language-plaintext highlighter-rouge">xgboost</code> package has been publicly available since mid-2015.
Initial releases (0.4, 0.6) carry “pre-release” markers.
The first production-ready release is XGBoost version 0.7 (aka 0.70), which is dated 1st of January, 2018.</p>

<p>This blog post walks through all major XGBoost releases from 0.7 to 1.7, with the intent of pinpointing any Scikit-Learn API changes, and making observations and comparisons from the end user perspective.
The focus is on handling categorical data.
Any measurable changes in predictive or computational performance are ignored. The general expectation is that each new major XGBoost release version is smarter and faster than all the previous ones.</p>

<h2 id="xgboost-version-07-v0721">XGBoost version 0.7 (v0.72.1)</h2>

<p>The “audit” dataset deals with a binary classification problem.</p>

<p>The label column is originally a two-valued integer, but it is translated into a two-valued string in order to make the experiment more challenging.</p>

<p>There are nine feature columns, three continuous and six categorical.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"audit.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">"Adjusted"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"Adjusted"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="s">"yes"</span> <span class="k">if</span> <span class="n">x</span> <span class="k">else</span> <span class="s">"no"</span><span class="p">))</span>

<span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Deductions"</span><span class="p">,</span> <span class="s">"Education"</span><span class="p">,</span> <span class="s">"Employment"</span><span class="p">,</span> <span class="s">"Gender"</span><span class="p">,</span> <span class="s">"Marital"</span><span class="p">,</span> <span class="s">"Occupation"</span><span class="p">]</span>
<span class="n">cont_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Age"</span><span class="p">,</span> <span class="s">"Income"</span><span class="p">,</span> <span class="s">"Hours"</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">cat_cols</span> <span class="o">+</span> <span class="n">cont_cols</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"Adjusted"</span><span class="p">]</span>
</code></pre></div></div>

<p>A good Scikit-Learn pipeline should start with an “initializer” step that ensures that the incoming data matrix is correct (eg. right columns, in the right order).</p>

<p>Both <code class="language-plaintext highlighter-rouge">sklearn_pandas.DataFrameMapper</code> and <code class="language-plaintext highlighter-rouge">sklearn.compose.ColumnTransformer</code> (meta-)transformers offer combined data matrix canonicalization and transformation functionality.
This experiment goes with the <code class="language-plaintext highlighter-rouge">DataFrameMapper</code> class, because it has simpler and more intuitive API, is purpose-built for dealing with <code class="language-plaintext highlighter-rouge">pandas.DataFrame</code> inputs and outputs, and works with any Scikit-Learn version.</p>

<p>Constructing an OHE-style initializer:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.decoration</span> <span class="kn">import</span> <span class="n">CategoricalDomain</span><span class="p">,</span> <span class="n">ContinuousDomain</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="k">def</span> <span class="nf">make_mapper</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">DataFrameMapper</span><span class="p">(</span>
    <span class="p">[([</span><span class="n">cont_col</span><span class="p">],</span> <span class="p">[</span><span class="n">ContinuousDomain</span><span class="p">()])</span> <span class="k">for</span> <span class="n">cont_col</span> <span class="ow">in</span> <span class="n">cont_cols</span><span class="p">]</span> <span class="o">+</span>
    <span class="p">[([</span><span class="n">cat_col</span><span class="p">],</span> <span class="p">[</span><span class="n">CategoricalDomain</span><span class="p">(),</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">int8</span><span class="p">)])</span> <span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">cat_cols</span><span class="p">]</span>
  <span class="p">,</span> <span class="n">input_df</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">df_out</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Numeric columns can be mapped as-is.
However, string and boolean columns must be transformed into numeric columns.</p>

<p>A data scientist is free to employ any third-party library, any string-to-numeric encoding algorithm here.
For example, the <a href="https://github.com/scikit-learn-contrib/category_encoders"><code class="language-plaintext highlighter-rouge">category_encoders</code></a> package provides a plethora of Scikit-Learn compatible transformers, which are often based on the state-of-the-art ML research publications.</p>

<p>The <code class="language-plaintext highlighter-rouge">scikit-learn</code> package is rather hapless in comparison.
In fact, the only option is the <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.OneHotEncoder</code> transformer, which performs a deep, two-level encoding of categorical columns (ie. first from string to ordinal integer, and then from ordinal integer to a list of binary indicators).</p>

<p>The other candidate, the <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.OrdinalEncoder</code> transformer fails in the vetting process, because it performs a shallow, one-level encoding (ie. from string to ordinal integer).
Unlike the LightGBM algorithm, the XGBoost algorithm is not programmed to handle such “partially” encoded columns.
If ordinal integers slip through, then they will be treated analogously to continuous integers, which qualifies as a serious mistake.</p>

<p><strong>Important</strong>: A pipeline where Scikit-Learn’s <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer is combined with an XGBoost estimator can only be applied to dense datasets, and never to sparse datasets.
In brief, the <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer is functionally restricted to two-state output.
This is sufficient for encoding dense datasets (ie. “on” and “off” states), but not sparse datasets (ie. “on”, “off” and “unknown” states).</p>

<p><strong>Important</strong>: A Scikit-Learn’s <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer and an XGBoost estimator are not compatible with one another in their default configurations.
In brief, the <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer produces sparse data matrices by default, where the <code class="language-plaintext highlighter-rouge">1</code> value represents the “on” state and the omitted value represents the “off” state. However, an XGBoost estimator (mis-)interprets omitted values as the “unknown” state.</p>

<p>The above notes have been discussed in detail in an earlier blog post about <a href="/blog/2022/04/12/onehot_encoding_sklearn_xgboost_pipeline/">one-hot encoding categorical features in Scikit-Learn XGBoost pipelines</a>.</p>

<p>There are two pipeline configurations that yield provably correct results:</p>

<ul>
  <li>Making OHE output dense - <code class="language-plaintext highlighter-rouge">[OneHotEncoder(sparse_output = False, dtype = numpy.int8), XGBModel()]</code>.</li>
  <li>Keeping OHE output as sparse, and instructing XGBoost to interpret some value (other than <code class="language-plaintext highlighter-rouge">float("NaN")</code>) as missing value - <code class="language-plaintext highlighter-rouge">[OneHotEncoder(sparse_output = True), XGBModel(missing = -999)]</code>.</li>
</ul>

<p>The first configuration is more robust than the second (easier to understand, less likely to break after a scheduled Scikit-Learn or XGBoost version update).
However, it involves a matrix densification operation, which can raise memory requirements by several orders of magnitude. When going this route, then the impact can be softened somewhat by changing the data type of output data matrix elements to some very small and cheap one such as <code class="language-plaintext highlighter-rouge">numpy.(u)int8</code> or <code class="language-plaintext highlighter-rouge">bool</code>.</p>

<p>Constructing an OHE-style XGBoost estimator:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>

<span class="k">def</span> <span class="nf">make_classifier</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">objective</span> <span class="o">=</span> <span class="s">"binary:logistic"</span><span class="p">,</span> <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">131</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p>The values of <code class="language-plaintext highlighter-rouge">XGBModel.n_estimators</code> and <code class="language-plaintext highlighter-rouge">XGBModel.max_depth</code> attributes are set to values that should let the XGBoost algorithm to run to exhaustion with this dataset.</p>

<p>Constructing and fitting a pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">mapper</span> <span class="o">=</span> <span class="n">make_mapper</span><span class="p">()</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">make_classifier</span><span class="p">()</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">PMMLPipeline</span><span class="p">([</span>
  <span class="p">(</span><span class="s">"mapper"</span><span class="p">,</span> <span class="n">mapper</span><span class="p">),</span>
  <span class="p">(</span><span class="s">"classifier"</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>The XGBoost classifier performs label encoding and decoding during <code class="language-plaintext highlighter-rouge">XGBClassifier.fit(X, y)</code> and <code class="language-plaintext highlighter-rouge">XGBClassifier.predict(X)</code> method calls similarly to Scikit-Learn classifiers.
The learned encoding is stored in the <code class="language-plaintext highlighter-rouge">XGBClassifier._le</code> attribute as a <code class="language-plaintext highlighter-rouge">sklearn.preprocessing.LabelEncoder</code> object.</p>

<p>Exporting the booster object:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span><span class="p">.</span><span class="n">_Booster</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">"Booster.bin"</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">XGBModel._Booster</code> attribute (not to be confused with the <code class="language-plaintext highlighter-rouge">XGBModel.booster</code> attribute!) holds a reference to the underlying <code class="language-plaintext highlighter-rouge">xgboost.Booster</code> object.</p>

<p>Saving the booster object into a file is the suggested way for moving XGBoost models between ML frameworks and applications.
Unfortunately, the only supported data format is a binary proprietary one, for which there is no public specification or documentation available.</p>

<p>When making pipeline configuration changes, then it is possible to monitor both its intended and unintended consequences by diffing booster files.
For example, when updating parameterizations, or replacing Scikit-Learn’s <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer with some third-party implementation, then the byte content of the booster file must remain the same.</p>

<h2 id="xgboost-versions-08-and-09-v082-v090">XGBoost versions 0.8 and 0.9 (v0.82, v0.90)</h2>

<p>Added the <a href="https://github.com/dmlc/xgboost/blob/v0.82/python-package/xgboost/sklearn.py#L246-L262"><code class="language-plaintext highlighter-rouge">XGBModel.save_model(fname)</code></a> method:</p>

<p>Exporting the booster object:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifier</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">"Booster.bin"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="xgboost-version-10-v102">XGBoost version 1.0 (v1.0.2)</h2>

<p>Added the <a href="https://github.com/dmlc/xgboost/blob/v1.0.2/python-package/xgboost/compat.py#L140-L161"><code class="language-plaintext highlighter-rouge">XGBoostLabelEncoder</code></a> class, and changed the type of the <code class="language-plaintext highlighter-rouge">XGBClassifier._le</code> attribute to it.</p>

<p>Exporting the booster object is now also possible in JSON data format:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note the ".json" filename extension!
</span><span class="n">classifier</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">"Booster.json"</span><span class="p">)</span>
</code></pre></div></div>

<p>XGBoost generates a “minified” JSON document, which does not contain any newlines, indentation or even intermittent whitespace.
Such a multi-megabyte, non-breaking line of text poses a serious challenge to most text editors.</p>

<p>Pretty-printing a booster JSON file to make it more human friendly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python -m json.tool &lt; Booster.json &gt; Booster-pretty.json
</code></pre></div></div>

<h2 id="xgboost-versions-11-and-12-v111-v121">XGBoost versions 1.1 and 1.2 (v1.1.1, v1.2.1)</h2>

<p>No changes.</p>

<h2 id="xgboost-version-13-v133">XGBoost version 1.3 (v1.3.3)</h2>

<p>Added the <a href="https://github.com/dmlc/xgboost/blob/v1.3.3/python-package/xgboost/sklearn.py#L814"><code class="language-plaintext highlighter-rouge">XGBClassifier.use_label_encoder</code></a> attribute.</p>

<p>The <code class="language-plaintext highlighter-rouge">XGBClassifier.fit(X, y)</code> method emits a user warning that label encoding should be done manually:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
</code></pre></div></div>

<p>This request goes against Scikit-Learn conventions that label management is the responsibility of the ML framework.</p>

<p>Extracting the label encoder:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="kn">from</span> <span class="nn">xgboost.compat</span> <span class="kn">import</span> <span class="n">XGBoostLabelEncoder</span>

<span class="k">def</span> <span class="nf">make_classifier</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="s">"binary:logistic"</span><span class="p">,</span> <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">131</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">use_label_encoder</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">make_classifier</span><span class="p">()</span>

<span class="n">xgb_le</span> <span class="o">=</span> <span class="n">XGBoostLabelEncoder</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Series</span><span class="p">(</span><span class="n">xgb_le</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s">"Adjusted"</span><span class="p">)</span>

<span class="n">classifier</span><span class="p">.</span><span class="n">_le</span> <span class="o">=</span> <span class="n">xgb_le</span>
</code></pre></div></div>

<p>The extracted label encoder is stored in the <code class="language-plaintext highlighter-rouge">XGBClassifier._le</code> attribute just like before.</p>

<p>It is clear from the XGBoost library source code that this attribute has retained its special status.
For example, the <a href="https://github.com/dmlc/xgboost/blob/v1.3.3/python-package/xgboost/sklearn.py#L931-L987"><code class="language-plaintext highlighter-rouge">XGBClassifier.predict(X)</code></a> method checks for its presence and, if found, automatically post-processes the raw prediction using its <code class="language-plaintext highlighter-rouge">inverse_transform(y)</code> method.</p>

<p>According to <a href="https://github.com/dmlc/xgboost/releases/tag/v1.3.0">XGBoost version 1.3 release notes</a>, the library now supports direct categorical splits.
This functionality is considered highly experimental. It is partially integrated into the low-level Python Learning API, but not into the high-level Scikit-Learn API.</p>

<p>A booster object that contains categorical splits cannot be saved in binary proprietary data format.</p>

<h2 id="xgboost-version-14-v142">XGBoost version 1.4 (v1.4.2)</h2>

<p>The booster JSON file contains an embedded feature map (lists of feature names and feature types).</p>

<p>When making predictions, then the XGBoost library uses this information to ensure that the new validation or testing datasets are structurally similar to the training dataset.
Scikit-Learn pipelines should pass such checks cleanly thanks to the “initializer” step.</p>

<h2 id="xgboost-version-15-v152">XGBoost version 1.5 (v1.5.2)</h2>

<p>Added the <a href="https://github.com/dmlc/xgboost/blob/v1.5.2/python-package/xgboost/sklearn.py#L401"><code class="language-plaintext highlighter-rouge">XGBModel.enable_categorical</code></a> attribute.</p>

<p>Starting from here, the support for direct categorical splits is available at all API levels.</p>

<p>Constructing a categorical-style initializer:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.decoration</span> <span class="kn">import</span> <span class="n">CategoricalDomain</span><span class="p">,</span> <span class="n">ContinuousDomain</span>

<span class="k">def</span> <span class="nf">make_mapper</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">DataFrameMapper</span><span class="p">(</span>
    <span class="p">[([</span><span class="n">cont_col</span><span class="p">],</span> <span class="p">[</span><span class="n">ContinuousDomain</span><span class="p">()])</span> <span class="k">for</span> <span class="n">cont_col</span> <span class="ow">in</span> <span class="n">cont_cols</span><span class="p">]</span> <span class="o">+</span>
    <span class="p">[([</span><span class="n">cat_col</span><span class="p">],</span> <span class="p">[</span><span class="n">CategoricalDomain</span><span class="p">(</span><span class="n">dtype</span> <span class="o">=</span> <span class="s">"category"</span><span class="p">)])</span> <span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">cat_cols</span><span class="p">]</span>
  <span class="p">,</span> <span class="n">input_df</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">df_out</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>A categorical column can be marked as such by simply casting it to the <a href="https://pandas.pydata.org/docs/reference/api/pandas.CategoricalDtype.html"><code class="language-plaintext highlighter-rouge">pandas.CategoricalDtype</code></a> data type.</p>

<p><strong>Important</strong>: A quick cast using the <code class="language-plaintext highlighter-rouge">category</code> data type alias is dataset-dependent.
If the same pipeline object is used for making predictions on different datasets, then the <code class="language-plaintext highlighter-rouge">dtype</code> argument must be a full-blown <code class="language-plaintext highlighter-rouge">CategoricalDtype</code> object, which enumerates all valid category levels ahead of time.</p>

<p>Removing Scikit-Learn’s <code class="language-plaintext highlighter-rouge">OneHotEncoder</code> transformer lifts all restrictions that were previously imposed by it.
Specifically, the same pipeline can now be applied to both dense and sparse datasets.</p>

<p>Constructing a categorical-style XGBoost estimator:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>

<span class="k">def</span> <span class="nf">make_classifier</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="s">"binary:logistic"</span><span class="p">,</span> <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">131</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">tree_method</span> <span class="o">=</span> <span class="s">"gpu_hist"</span><span class="p">,</span> <span class="n">enable_categorical</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">use_label_encoder</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">enable_categorical</code> argument acts as a double confirmation mechanism.
If not explicitly set to <code class="language-plaintext highlighter-rouge">True</code>, then the fit method shall fail with the following value error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback (most recent call last):
  File "train-categorical.py", line 45, in &lt;module&gt;
    pipeline.fit(X, y)
  File "/usr/local/lib/python3.9/site-packages/sklearn/pipeline.py", line 406, in fit
    self._final_estimator.fit(Xt, y, **fit_params_last_step)
  ...
  File "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py", line 1245, in &lt;lambda&gt;
    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),
  ...
  File "/usr/local/lib/python3.9/site-packages/xgboost/data.py", line 772, in dispatch_data_backend
    return _from_pandas_df(data, enable_categorical, missing, threads,
  File "/usr/local/lib/python3.9/site-packages/xgboost/data.py", line 312, in _from_pandas_df
    data, feature_names, feature_types = _transform_pandas_df(
  File "/usr/local/lib/python3.9/site-packages/xgboost/data.py", line 256, in _transform_pandas_df
    _invalid_dataframe_dtype(data)
  File "/usr/local/lib/python3.9/site-packages/xgboost/data.py", line 236, in _invalid_dataframe_dtype
    raise ValueError(msg)
ValueError: DataFrame.dtypes for data must be int, float, bool or category.  When
categorical type is supplied, DMatrix parameter `enable_categorical` must
be set to `True`. Invalid columns:Deductions, Education, Employment, Gender, Marital, Occupation
</code></pre></div></div>

<p>This pipeline refactoring from “OHE style” to “categorical style” is provably correct, because these two configurations produce identical decision tree ensembles when fitted on the same platform using the same <code class="language-plaintext highlighter-rouge">tree_method</code> tree param.</p>

<p>The above statement is trivial to verify by diffing PMML files:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ diff XGBoostAudit-OHE.pmml XGBoostAudit-categorical.pmml
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">diff</code> output shows changes to two lines of text.
First, the <a href="https://dmg.org/pmml/v4-4-1/Header.html#xsdElement_Header"><code class="language-plaintext highlighter-rouge">Header</code></a> element contains a different document creation timestamp.
Second, the data type of the “Deductions” field has changed from <code class="language-plaintext highlighter-rouge">boolean</code> to <code class="language-plaintext highlighter-rouge">string</code>.</p>

<p>The technical cause for the unexpected data type change remains unexplained.
Perhaps there is a slight incompatibility along the <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">sklearn_pandas</code> and <code class="language-plaintext highlighter-rouge">pandas</code> library chain, which sorts itself out after a couple of package version updates.</p>

<p>In contrast, the above statement (about two decision tree ensembles being identical) is impossible to verify by diffing booster JSON files.</p>

<p>The booster JSON file references feature map entries by index.
However, this pipeline refactoring caused the embedded feature map to collapse in size (from 50 entries to 9 entries), thereby introducing large systematic shifts into index values.
For example, the “Education” field was mapped to a feature map entry range <code class="language-plaintext highlighter-rouge">[5, 20]</code> (ie. a list of 16 binary indicator features), but is now mapped to a sole feature map entry <code class="language-plaintext highlighter-rouge">[4]</code> (ie. a single categorical feature).</p>

<p>The <code class="language-plaintext highlighter-rouge">diff</code> output shows that the booster JSON file has been rewritten.
While technically correct, this observation is completely unhelpful when trying to understand the bigger picture.</p>

<h2 id="xgboost-version-16-v162">XGBoost version 1.6 (v1.6.2)</h2>

<p>Deprecated the use of the <code class="language-plaintext highlighter-rouge">XGBClassifier.use_label_encoder</code> attribute, and added the <a href="https://github.com/dmlc/xgboost/blob/v1.6.2/python-package/xgboost/sklearn.py#L521"><code class="language-plaintext highlighter-rouge">XGBModel.max_cat_to_onehot</code></a> attribute.</p>

<p>Evidently, XGBoost is quickly getting better at categorical splits.
The OHE-based partitioning is considered outdated. The new default is <a href="https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html#optimal-partitioning">set-based aka optimal partitioning</a>.</p>

<p>The <code class="language-plaintext highlighter-rouge">max_cat_tp_onehot</code> attribute allows the data scientist to choose one partitioning algorithm over the other.</p>

<p>The <a href="https://github.com/dmlc/xgboost/blob/v1.6.2/src/tree/param.h#L112-L115">default value of the <code class="language-plaintext highlighter-rouge">max_cat_to_onehot</code> tree param is 4</a>.
Typical values range from 2 to 20. Setting the value to an arbitrarily large number (eg. over 10’000) will effectively disable the set-based aka optimal partitioning.</p>

<p>Granted, one-hot encoding does not work particularly well with medium- and high-cardinality categorical features, especially if there are second-order effects involved (eg. sets of closely related category levels).</p>

<p>One-hot encoding faces additional headwinds with decision tree models, because it identifies and isolates significant category levels one by one, leading to long and thin branch growth.
For example, a member decision tree whose maximum depth is limited to 6 (the default value for the <code class="language-plaintext highlighter-rouge">max_depth</code> tree param) will be fully developed after coming in contact with a categorical feature that has six or more significant category levels.</p>

<p>Exporting the booster object is now also possible in <a href="https://ubjson.org/">Universal Binary JSON</a> (UBJSON) data format:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note the ".ubj" filename extension!
</span><span class="n">classifier</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="s">"Booster.ubj"</span><span class="p">)</span>
</code></pre></div></div>

<p>UBJSON is a binary data serialization format for JSON-compatible data structures.
It aims to improve data reading and writing speeds, and reduce data size. These capabilities are highly relevant, as the size of booster JSON files can reach tens to hundreds of megabytes.</p>

<h2 id="xgboost-version-17-v173">XGBoost version 1.7 (v1.7.3)</h2>

<p>Added the <a href="https://github.com/dmlc/xgboost/blob/v1.7.3/python-package/xgboost/sklearn.py#L577"><code class="language-plaintext highlighter-rouge">XGBModel.max_cat_threshold</code></a> attribute.</p>

<p>This attribute limits the maximum set size for the set-based aka optimal partitioning.</p>

<p>The <a href="https://github.com/dmlc/xgboost/blob/v1.7.3/src/tree/param.h#L118-L123">default value of the <code class="language-plaintext highlighter-rouge">max_cat_threshold</code> tree param is 64</a>.</p>

<p>The XGBoost algorithm will consider a direct categorical split only if the number of category levels for a categorical feature has dropped below this value.
For example, the <a href="https://facts.usps.com/42000-zip-codes/">USPS ZIP Code has over 40’000 codes</a>.
When used as a categorical feature, then it would be useless to attempt a categorical split at a federal level (eg. sending 10’000 codes to the left and 30’000 codes to the right) or even at a state level. However, it would be very useful to attempt the same at county or city levels (eg. sending 10 codes to the left and 30 to the right).</p>

<p>The structure of member decision trees reflects how the original heterogeneous dataset is being divided into smaller and more homogenous subsets.
Continuous and low-cardinality categorical features (see the <code class="language-plaintext highlighter-rouge">max_cat_tp_onehot</code> tree param!) can appear at every branching level.
In contrast, medium- and high-cardinality categorical features initially stay on the sidelines. They appear gradually at deeper branching levels, where the details matter.</p>

<h2 id="pmml">PMML</h2>

<p>The <a href="https://github.com/jpmml/jpmml-xgboost">JPMML-XGBoost</a> library converts booster objects to the standardized Predictive Model Markup Language (PMML) representation.
This library supports all XGBoost versions and data formats (eg. Binary, JSON, UBJSON), and can be used either as a standalone command-line application or as an ML framework plugin.</p>

<p>Getting started with PMML is easy.
Simply replace the default Scikit-Learn’s pipeline class with the <code class="language-plaintext highlighter-rouge">sklearn2pmml.pipeline.PMMLPipeline</code> class, and then pass the fitted pipeline object to the <code class="language-plaintext highlighter-rouge">sklearn2pmml.sklearn2pmml</code> utility function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn2pmml</span> <span class="kn">import</span> <span class="n">sklearn2pmml</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.pipeline</span> <span class="kn">import</span> <span class="n">PMMLPipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">PMMLPipeline</span><span class="p">([</span>
  <span class="p">(</span><span class="s">"mapper"</span><span class="p">,</span> <span class="n">mapper</span><span class="p">),</span>
  <span class="p">(</span><span class="s">"classifier"</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Disable PMML optimizations
</span><span class="n">pipeline</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">compact</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s">"XGBoostAudit.pmml"</span><span class="p">)</span>
</code></pre></div></div>

<p>The quality of PMML documents depends on the quality and completeness of the XGBoost model schema information.
For example, XGBoost versions 1.3 through 1.7 do not store category level values in the embedded feature map. It is impossible to generate meaningful categorical splits in such a situation, unless the embedded feature map is overriden with a more sophisticated external feature map.</p>

<p>The integration is much deeper within the JPMML ecosystem, as various JPMML platform libraries (eg. JPMML-SkLearn, JPMML-R, JPMML-SparkML) present the model schema as a live <code class="language-plaintext highlighter-rouge">org.jpmml.converter.Schema</code> object.</p>

<p>The JPMML-XGBoost library can expose one XGBoost model in different ways.
A PMML document can be optimized for explainability by expanding all decision tree leaves and maintaining complex feature values until the end. For example, generating <a href="https://dmg.org/pmml/v4-4-1/TreeModel.html#xsdGroup_PREDICATE">predicate elements</a> that operate with temporal feature values (eg. <code class="language-plaintext highlighter-rouge">&lt;SimplePredicate field="lastSeenDate" operator="lessOrEqual" value="2022-12-31"/&gt;</code>).
Alternatively, a PMML document can be optimized for resource efficiency by compacting and simplifying it.</p>

<p>The conversion process can be guided using conversion options.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li>“Audit” dataset: <a href="https://openscoring.io/resources/data/audit.csv"><code class="language-plaintext highlighter-rouge">audit.csv</code></a></li>
  <li>Python scripts: <a href="https://openscoring.io/resources/2023-02-06/train-OHE.py"><code class="language-plaintext highlighter-rouge">train-OHE.py</code></a> (XGBoost version 1.0 and newer) and <a href="https://openscoring.io/resources/2023-02-06/train-categorical.py"><code class="language-plaintext highlighter-rouge">train-categorical.py</code></a> (XGBoost version 1.6 and newer)</li>
</ul>

</div>



  </main>

  <footer class="footer">
  <div class="container">
    <div class="mb-6">© 2022 - Openscoring</div>

    <div class="mb-6">
      <ul class="flex gap-4 list-none">
        <li>
          <a href="mailto:info@openscoring.io" aria-label="email">
            <img src="/assets/images/email_round.svg" width="48" height="48" alt="Contact Openscoring">
          </a>
        </li>
        <li>
          <a href="https://twitter.com/openscoring" target="_blank" aria-label="twitter">
            <img src="/assets/images/twitter_round.svg" width="48" height="48" alt="Follow Openscoring on Twitter">
          </a>
        </li>
      </ul>
    </div>
  </div>
</footer>

  <script>
  var sc_project=11704106;
  var sc_security="a7d1bf16"; 
  var sc_invisible=1; 
  var sc_remove_link=1; 
</script>

<script src="https://www.statcounter.com/counter/counter.js" async></script>

<noscript>
  <div class="statcounter">
    <img class="statcounter" src="https://c.statcounter.com/11704106/0/a7d1bf16/1/" alt="Web Analytics Made Easy - Statcounter" referrerPolicy="no-referrer-when-downgrade">
  </div>
</noscript>
</body>

</html>
