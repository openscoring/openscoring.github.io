<!DOCTYPE html>
<html lang="en">

<head>
  <script src="https://www.googletagmanager.com/gtag/js?id=G-1XC1KCZX17" async></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'G-1XC1KCZX17');
</script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
  <meta name="format-detection" content="telephone=no">
  <meta name="theme-color" content="#ffffff">
  <meta name="msapplication-navbutton-color" content="#ffffff">
  <meta name="apple-mobile-web-app-status-bar-style" content="#ffffff">

  <meta name="description" content="">
  <meta name="keywords" content="scikit-learn sklearn2pmml statsmodels tuning">
  <meta name="author" content="vruusmann">

  <title>Training Scikit-Learn GridSearchCV StatsModels pipelines - Openscoring</title>

  <meta name="robots" content="index, follow, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
  <link rel="canonical" href="https://openscoring.io/blog/2023/10/15/sklearn_statsmodels_gridsearchcv_pipeline/">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Training Scikit-Learn GridSearchCV StatsModels pipelines - Openscoring">
  <meta property="og:url" content="/blog/2023/10/15/sklearn_statsmodels_gridsearchcv_pipeline/">
  <meta property="og:site_name" content="Openscoring">
  <meta property="og:updated_time" content="2023-10-15 00:00:00 +0300">
  <meta property="article:published_time" content="2023-10-15 00:00:00 +0300">
  <meta property="article:modified_time" content="2023-10-15 00:00:00 +0300">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Training Scikit-Learn GridSearchCV StatsModels pipelines - Openscoring">
  <meta name="twitter:label1" content="Written by">
  <meta name="twitter:data1" content="Villu Ruusmann">
  <meta name="twitter:label2" content="Time to read">
  <meta name="twitter:data2" content="Less than a minute">

  <link rel="icon" href="/assets/images/fa-150x150.png" sizes="32x32">
  <link rel="icon" href="/assets/images/fa.png" sizes="192x192">
  <link rel="apple-touch-icon" href="/assets/images/fa.png">
  <link rel="stylesheet" href="/assets/css/main.css">
  <meta name="msapplication-TileImage" content="h/assets/images/fa.png">
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<body>
  <header class="header">
  <div class="container">
    <nav class="nav--main">
      <a class="logo" href="/" aria-label="home">
        <img src="/assets/images/logo.svg" alt="" width="35" height="32" loading="lazy">
        <span>Openscoring</span>
      </a>

      <input type="checkbox" id="nav__toggle--main" class="nav__toggle">
      <label for="nav__toggle--main">Menu<span></span></label>

      <div class="menu">
        <ul id="menu--main">
          <li><a href="/#overview" aria-current="page">Overview</a></li>
          <li><a href="/#products" aria-current="page">Products</a></li>
          <li><a href="/#licensing" aria-current="page">Licensing</a></li>
          <li><a href="/#consulting" aria-current="page">Consulting</a></li>
        </ul>
      </div>

      <label for="nav__toggle--main" class="overlay"></label>

      <a href="/blog/" class="btn btn--small">Blog</a>
    </nav>
  </div>
</header>

  <main class="container mt-26">
    <h1>Training Scikit-Learn GridSearchCV StatsModels pipelines</h1>
  
<div class="post">
  <p>The <a href="https://github.com/jpmml/sklearn2pmml"><code class="language-plaintext highlighter-rouge">sklearn2pmml</code></a> package provides Scikit-Learn style wrappers for StatsModels classification, ordinal classification and regression models:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">sklearn2pmml.statsmodels.StatsModelsClassifier</code></li>
  <li><code class="language-plaintext highlighter-rouge">sklearn2pmml.statsmodels.StatsModelsOrdinalClassifier</code></li>
  <li><code class="language-plaintext highlighter-rouge">sklearn2pmml.statsmodels.StatsModelsRegressor</code></li>
</ul>

<p>Wrapper classes are maximally generic by design.
For example, the <code class="language-plaintext highlighter-rouge">StatsModelsRegressor</code> wrapper can accommodate any StatsModels regression model, any StatsModels version.</p>

<p>This genericity is achieved using <a href="https://docs.python.org/3/glossary.html#term-argument">Python Arbitrary Keyword Arguments</a> (aka <code class="language-plaintext highlighter-rouge">**kwargs</code>) mechanisms.
Wrapper class methods accept any keyword arguments. They get packed into a singular <code class="language-plaintext highlighter-rouge">dict</code>-type helper param, which is then dispatched to the right StatsModels model method at the right time.</p>

<p>Such “param aggregation” approach works fine with most common Scikit-Learn workflows.
However, it falls short with advanced workflows, where individual Scikit-Learn estimators need to be queried and/or updated on a param-by-param basis.</p>

<p>The most prominent example is hyperparameter tuning.</p>

<h2 id="training">Training</h2>

<p>A hyperparameter tuning workflow can be summarized as follows:</p>
<ol>
  <li>The end user provides a template estimator object and declares all its tunable params.</li>
  <li>The tuner makes a clone of the template estimator object and updates the initial values of one or more tunable params with new values.</li>
  <li>The tuner scores the updated estimator object. The updated estimator object is kept if it out-scores the previous best estimator object, and is discarded otherwise.</li>
  <li>The tuner repeats stages #2 and #3 until the stop criterion is met.</li>
</ol>

<p>In the second workflow stage, candidate Scikit-Learn estimators are fabricated using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.base.clone.html"><code class="language-plaintext highlighter-rouge">sklearn.base.clone</code></a> utility function.
The fabrication algorithm constructs a new unfitted estimator object irrespective if the “template” was a fitted or unfitted estimator object (ie. “selective clone” rather than “full clone” semantics).</p>

<p>The tunable param set is determined via the <code class="language-plaintext highlighter-rouge">get_params</code> method.
The default implementation returns the constructor params of the estimator class.</p>

<p>Wrapper classes declare two named params <code class="language-plaintext highlighter-rouge">model_class</code> and <code class="language-plaintext highlighter-rouge">fit_intercept</code>.
They are typically set by a human expert depending on the nature of the modeling task.
However, there are no technical restrictions to varying them using grid search means in order to conduct a quick AutoML-like StatsModels model selection experiment:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.statsmodels</span> <span class="kn">import</span> <span class="n">StatsModelsRegressor</span>
<span class="kn">from</span> <span class="nn">statsmodels.api</span> <span class="kn">import</span> <span class="n">GLM</span><span class="p">,</span> <span class="n">OLS</span><span class="p">,</span> <span class="n">WLS</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_statsmodels_pipeline</span><span class="p">(</span><span class="n">StatsModelsRegressor</span><span class="p">(</span><span class="n">OLS</span><span class="p">))</span>

<span class="n">ctor_params_grid</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">"regressor__model_class"</span> <span class="p">:</span> <span class="p">[</span><span class="n">GLM</span><span class="p">,</span> <span class="n">OLS</span><span class="p">,</span> <span class="n">WLS</span><span class="p">],</span>
  <span class="s">"regressor__fit_intercept"</span> <span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">ctor_params_grid</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">tuner</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">tuner</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">)</span>
</code></pre></div></div>

<p>Any attempt to call the <code class="language-plaintext highlighter-rouge">GridSearchCV.fit(X, y)</code> method with an expanded param grid shall fail with a value error.</p>

<h3 id="task-specific-subclassing">Task-specific subclassing</h3>

<p>The canonical approach to enabling tunable params is by declaring them one-by-one as constructor params, and assigning them to Python class attributes (with exactly the same name) in the constructor body.</p>

<p>For example, defining a wrapper subclass for tuning the <code class="language-plaintext highlighter-rouge">alpha</code> and <code class="language-plaintext highlighter-rouge">L1_wt</code> params of the <a href="https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html"><code class="language-plaintext highlighter-rouge">OLS.fit_regularized</code></a> method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TunableStatsModelsRegressor</span><span class="p">(</span><span class="n">StatsModelsRegressor</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">L1_wt</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">init_params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">model_class</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="o">**</span><span class="n">init_params</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">L1_wt</span> <span class="o">=</span> <span class="n">L1_wt</span>

  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">L1_wt</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">L1_wt</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<h3 id="task-agnostic-subclassing">Task-agnostic subclassing</h3>

<p>Code reusability can be improved by aggregating all tunable params into a singular <code class="language-plaintext highlighter-rouge">dict</code>-type helper param.</p>

<p>However, this approach qualifies as a hack, because it takes advantage of the notion that the tuner is willing to perform update operations also on virtual params.
Such loophole exists in Scikit-Learn versions 1.0.X through 1.2.X. It may be closed in newer versions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TunableStatsModelsRegressor</span><span class="p">(</span><span class="n">StatsModelsRegressor</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">tune_params</span> <span class="o">=</span> <span class="p">{},</span> <span class="o">**</span><span class="n">init_params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">model_class</span> <span class="o">=</span> <span class="n">model_class</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="o">**</span><span class="n">init_params</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tune_params</span> <span class="o">=</span> <span class="n">tune_params</span>

  <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="n">super_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="n">k</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">).</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"model_class"</span><span class="p">,</span> <span class="s">"fit_intercept"</span><span class="p">,</span> <span class="s">"tune_params"</span><span class="p">]])</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">super_params</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">"tune_params"</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">tune_params</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<p>The above class self-reports <code class="language-plaintext highlighter-rouge">model_class</code>, <code class="language-plaintext highlighter-rouge">fit_intercept</code> and <code class="language-plaintext highlighter-rouge">tune_params</code> params, but is open to perform an update operation on any param.</p>

<p>This “discrepancy” is powered by a custom <code class="language-plaintext highlighter-rouge">set_params</code> method.
Update operations that target self-reported params are dispatched to the superclass’ <code class="language-plaintext highlighter-rouge">set_params</code> method.
All other update operations are treated as Python item assignment operations against the <code class="language-plaintext highlighter-rouge">tune_params</code> param.</p>

<p>The combined StatsModels model selection and model hyperparameter tuning experiment succeeds with either flavour of <code class="language-plaintext highlighter-rouge">TunableStatsModelsRegressor</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_statsmodels_pipeline</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">(</span><span class="n">OLS</span><span class="p">))</span>

<span class="n">ctor_params_grid</span> <span class="o">=</span> <span class="p">{</span>
	<span class="s">"regressor__fit_intercept"</span> <span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">regfit_params_grid</span> <span class="o">=</span> <span class="p">{</span>
	<span class="s">"regressor__alpha"</span> <span class="p">:</span> <span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span>
	<span class="s">"regressor__L1_wt"</span> <span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">ctor_params_grid</span><span class="p">,</span> <span class="o">**</span><span class="n">regfit_params_grid</span><span class="p">},</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">tuner</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">regressor__fit_method</span> <span class="o">=</span> <span class="s">"fit_regularized"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">tuner</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="deployment">Deployment</h2>

<p>Hyperparameter tuning is 100% training-time phenomenon, which should not cause any complications at later model lifecycle phases.
Any kind of subclassing violates this principle, because it creates a need to package and distribute model class definition(s) alongside the model object.</p>

<p>When the model deployment happens in the Python environment, then the solution is to transform the <code class="language-plaintext highlighter-rouge">TunableStatsModelsRegressor</code> object into a new object for which there is a stable and reusable class definition available.</p>

<p>The <code class="language-plaintext highlighter-rouge">TunableStatsModelsRegressor</code> class does not hold any fitted state beyond that of its <code class="language-plaintext highlighter-rouge">StatsModelsRegressor</code> parent class.
Therefore, as another hack, the model object can be made easily recognizable to its future users by simply re-assigning its <code class="language-plaintext highlighter-rouge">__class__</code> attribute:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">joblib</span>

<span class="n">best_pipeline</span> <span class="o">=</span> <span class="n">tuner</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="n">best_regressor</span> <span class="o">=</span> <span class="n">best_pipeline</span><span class="p">.</span><span class="n">_final_estimator</span>
<span class="n">best_regressor</span><span class="p">.</span><span class="n">__class__</span> <span class="o">=</span> <span class="n">StatsModelsRegressor</span>

<span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">best_pipeline</span><span class="p">,</span> <span class="s">"GridSearchAuto.pkl"</span><span class="p">)</span>
</code></pre></div></div>

<p>The proper way of model class conversion is to re-fit a new pipeline object using the best param set.
If the latter is a mix of constructor and fit method params, then it must be partitioned into two disjoint subsets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">joblib</span>

<span class="n">best_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">tuner</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="n">best_pipeline</span> <span class="o">=</span> <span class="n">make_statsmodels_pipeline</span><span class="p">(</span><span class="n">StatsModelsRegressor</span><span class="p">(</span><span class="n">OLS</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">best_params</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">"regressor__fit_intercept"</span><span class="p">)))</span>
<span class="n">best_pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">best_params</span><span class="p">,</span> <span class="n">regressor__fit_method</span> <span class="o">=</span> <span class="s">"fit_regularized"</span><span class="p">)</span>

<span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">best_pipeline</span><span class="p">,</span> <span class="s">"GridSearchAuto.pkl"</span><span class="p">)</span>
</code></pre></div></div>

<p>Model deployment in non-Python environments might seem impossible due to extensive StatsModels, Scikit-Learn and Python API dependencies.</p>

<p>No worries, because the <a href="https://github.com/jpmml">Java PMML API</a> software project provides a full stack of Java tools and libraries for untangling and converting arbitrary complexity Python ML artifacts to the Predictive Model Markup Language (PMML) representation.
Dealing with the <code class="language-plaintext highlighter-rouge">best_pipeline</code> object in a fully automated fashion does not pose any substantial challenge.</p>

<p>There is one minor configuration issue related to the fact that the <a href="https://github.com/jpmml-sklearn">JPMML-SkLearn</a> library recognizes and supports wrapper classes, but not their ad hoc subclasses.</p>

<p>It is possible to avoid tedious model class conversion operation by declaring model class equivalence (ie. “treat <code class="language-plaintext highlighter-rouge">TunableStatsModelsRegressor</code> objects the same as <code class="language-plaintext highlighter-rouge">StatsModelsRegressor</code> objects”) using a custom class mapping:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn2pmml</span> <span class="kn">import</span> <span class="n">load_class_mapping</span><span class="p">,</span> <span class="n">make_class_mapping_jar</span><span class="p">,</span> <span class="n">sklearn2pmml</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.util</span> <span class="kn">import</span> <span class="n">fqn</span>

<span class="n">default_mapping</span> <span class="o">=</span> <span class="n">load_class_mapping</span><span class="p">()</span>

<span class="c1"># Map the ad hoc subclass to the same JPMML-SkLearn converter class as the parent class
</span><span class="n">statsmodels_mapping</span> <span class="o">=</span> <span class="p">{</span>
	<span class="n">fqn</span><span class="p">(</span><span class="n">TunableStatsModelsRegressor</span><span class="p">)</span> <span class="p">:</span> <span class="n">default_mapping</span><span class="p">[</span><span class="n">fqn</span><span class="p">(</span><span class="n">StatsModelsRegressor</span><span class="p">)]</span>
<span class="p">}</span>

<span class="n">extension_jar</span> <span class="o">=</span> <span class="s">"TunableStatsModelsRegressor.jar"</span>

<span class="n">make_class_mapping_jar</span><span class="p">(</span><span class="n">statsmodels_mapping</span><span class="p">,</span> <span class="n">extension_jar</span><span class="p">)</span>

<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">tuner</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="s">"GridSearchAuto.pmml"</span><span class="p">,</span> <span class="n">user_classpath</span> <span class="o">=</span> <span class="p">[</span><span class="n">extension_jar</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="resources">Resources</h2>

<ul>
  <li>Dataset: <a href="https://openscoring.io/resources/data/auto.csv"><code class="language-plaintext highlighter-rouge">auto.csv</code></a></li>
  <li>Python script: <a href="https://openscoring.io/resources/2023-10-15/train.py"><code class="language-plaintext highlighter-rouge">train.py</code></a></li>
</ul>

</div>



  </main>

  <footer class="footer">
  <div class="container">
    <div class="mb-6">© 2022 - Openscoring</div>

    <div class="mb-6">
      <ul class="flex gap-4 list-none">
        <li>
          <a href="mailto:info@openscoring.io" aria-label="email">
            <img src="/assets/images/email_round.svg" width="48" height="48" alt="Contact Openscoring">
          </a>
        </li>
        <li>
          <a href="https://twitter.com/openscoring" target="_blank" aria-label="twitter">
            <img src="/assets/images/twitter_round.svg" width="48" height="48" alt="Follow Openscoring on Twitter">
          </a>
        </li>
      </ul>
    </div>
  </div>
</footer>

  <script>
  var sc_project=11704106;
  var sc_security="a7d1bf16"; 
  var sc_invisible=1; 
  var sc_remove_link=1; 
</script>

<script src="https://www.statcounter.com/counter/counter.js" async></script>

<noscript>
  <div class="statcounter">
    <img class="statcounter" src="https://c.statcounter.com/11704106/0/a7d1bf16/1/" alt="Web Analytics Made Easy - Statcounter" referrerPolicy="no-referrer-when-downgrade">
  </div>
</noscript>
</body>

</html>
