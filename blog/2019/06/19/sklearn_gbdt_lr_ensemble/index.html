<!DOCTYPE html>
<html lang="en">

<head>
  <script src="https://www.googletagmanager.com/gtag/js?id=G-1XC1KCZX17" async></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'G-1XC1KCZX17');
</script>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
  <meta name="format-detection" content="telephone=no">
  <meta name="theme-color" content="#ffffff">
  <meta name="msapplication-navbutton-color" content="#ffffff">
  <meta name="apple-mobile-web-app-status-bar-style" content="#ffffff">

  <meta name="description" content="">
  <meta name="keywords" content="scikit-learn lightgbm xgboost sklearn2pmml data-categorical">
  <meta name="author" content="vruusmann">

  <title>Extending Scikit-Learn with GBDT+LR ensemble models - Openscoring</title>

  <meta name="robots" content="index, follow, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
  <link rel="canonical" href="https://openscoring.io/blog/2019/06/19/sklearn_gbdt_lr_ensemble/">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Extending Scikit-Learn with GBDT+LR ensemble models - Openscoring">
  <meta property="og:url" content="/blog/2019/06/19/sklearn_gbdt_lr_ensemble/">
  <meta property="og:site_name" content="Openscoring">
  <meta property="og:updated_time" content="2019-06-19 00:00:00 +0300">
  <meta property="article:published_time" content="2019-06-19 00:00:00 +0300">
  <meta property="article:modified_time" content="2019-06-19 00:00:00 +0300">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Extending Scikit-Learn with GBDT+LR ensemble models - Openscoring">
  <meta name="twitter:label1" content="Written by">
  <meta name="twitter:data1" content="Villu Ruusmann">
  <meta name="twitter:label2" content="Time to read">
  <meta name="twitter:data2" content="Less than a minute">

  <link rel="icon" href="/assets/images/fa-150x150.png" sizes="32x32">
  <link rel="icon" href="/assets/images/fa.png" sizes="192x192">
  <link rel="apple-touch-icon" href="/assets/images/fa.png">
  <link rel="stylesheet" href="/assets/css/main.css">
  <meta name="msapplication-TileImage" content="h/assets/images/fa.png">
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>

<body>
  <header class="header">
  <div class="container">
    <nav class="nav--main">
      <a class="logo" href="/" aria-label="home">
        <img src="/assets/images/logo.svg" alt="" width="35" height="32" loading="lazy">
        <span>Openscoring</span>
      </a>

      <input type="checkbox" id="nav__toggle--main" class="nav__toggle">
      <label for="nav__toggle--main">Menu<span></span></label>

      <div class="menu">
        <ul id="menu--main">
          <li><a href="/#overview" aria-current="page">Overview</a></li>
          <li><a href="/#products" aria-current="page">Products</a></li>
          <li><a href="/#licensing" aria-current="page">Licensing</a></li>
          <li><a href="/#consulting" aria-current="page">Consulting</a></li>
        </ul>
      </div>

      <label for="nav__toggle--main" class="overlay"></label>

      <a href="/blog/" class="btn btn--small">Blog</a>
    </nav>
  </div>
</header>

  <main class="container mt-26">
    <h1>Extending Scikit-Learn with GBDT+LR ensemble models</h1>
  
<div class="post">
  <p>Logistic regression (LR) is often the go-to choice for binary classification.
Owing to extreme simplicity, LR models are fast to train and easy to deploy, and readily lend themselves for human interpretation.</p>

<p>The predictive performance of LR models depends on the quality and sophistication of data pre-processing, specifically feature engineering.
There are two major work areas.
First, delineating and generating the intended feature space. LR algorithms operate on the feature space they are given.
They are not designed to independently discover non-linearities along individual dimensions, or interactions between multiple dimensions.
Second, filtering down the feature space.
Specialized LR algorithms can prioritize and eliminate dimensions using regularization. However, the most common ones estimate coefficients for all dimensions of the feature space.</p>

<p>Facebook Research has demonstrated how feature engineering can be automated using a gradient boosted decision tree (GBDT) model: <a href="https://research.facebook.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/">Practical Lessons from Predicting Clicks on Ads at Facebook</a></p>

<p>The idea is to train a GBDT model on a raw feature space and collect and examine the “decision paths” of its member decision tree models.
A decision path which operates on a single feature can be regarded as a non-linear transformation on it (eg. binning a continuous feature to a pseudo-categorical feature). A decision path which operates on multiple features can be regarded as an interaction between them.</p>

<p>GBDT algorithms typically grow shallow decision trees.
Shallow trees contain short decision paths, which generally lead to easily interpretable derived features.</p>

<p>A boosting algorithm can be swapped for a bagging algorithm.
Random forest (RF) algorithms typically grow much deeper decision trees.
Longer decision paths lead to more complex derived features (eg. interactions between multiple non-linearly transformed features), which lose in interpretability but gain in information content.
For example, discovering cliffs and other anomalies in the decision space by observing which derived features become associated with extreme node scores.</p>

<h2 id="scikit-learn-perspective">Scikit-Learn perspective</h2>

<p>Scikit-Learn documentation dedicates a separate page to GBDT plus LR (GBDT+LR) ensemble models: <a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html">Feature transformations with ensembles of trees</a></p>

<p>While the concept and its implementation are discussed in great detail, there is no reusable GBDT+LR estimator available within Scikit-Learn.
Interested parties are either expected to copy-paste the example code, or rely on third-party libraries.</p>

<p>The <a href="https://github.com/jpmml/sklearn2pmml"><code class="language-plaintext highlighter-rouge">sklearn2pmml</code></a> package version 0.47.0 introduced <code class="language-plaintext highlighter-rouge">sklearn2pmml.ensemble.GBDTLRClassifier</code> and  <code class="language-plaintext highlighter-rouge">sklearn2pmml.ensemble.GBDTLMRegressor</code> ensemble models to address this deficiency.</p>

<h2 id="pmml-perspective">PMML perspective</h2>

<p>The Predictive Model Markup Language (PMML) provides standardized data structures for representing all common data pre- and post-processing operations and model types, including the GBDT model type and the LR model type.</p>

<p>If all the parts of a GBDT+LR model are PMML compatible, then it should follow that the GBDT+LR model itself is PMML compatible too?
The answer is a definite yes. Better yet, the PMML representation of a GBDT+LR model is reducible to an ordinary GBDT model, which leads to significant conversion- and run-time savings.</p>

<p>The reduction is based on the realization that GBDT+LR is a mechanism for replacing original GBDT leaf node scores with LR coefficients (and the GBDT base score with the LR intercept).
Scikit-Learn does not provide an API for modifying fitted decision trees.
The workaround is to make individual leaf nodes addressable using the one-hot encoding approach (the <code class="language-plaintext highlighter-rouge">OneHotEncoder.categories</code> attribute is a list of arrays; the size of the list equals the number of decision trees in the GBDT; the size of each array equals the number of leaf nodes in the corresponding decision tree), and then assigning a new score to each address (the <code class="language-plaintext highlighter-rouge">LogisticRegression.coef_</code> attribute is an array whose size equals the flat-mapped size of the <code class="language-plaintext highlighter-rouge">OneHotEncoder.categories</code> attribute).</p>

<p>The PMML representation does not need such layer of indirection, because it is possible to replace leaf node scores in place.</p>

<p>The <a href="https://github.com/jpmml/jpmml-model">JPMML-Model</a> library provides Visitor API for traversing, updating and transforming PMML class model objects.
In the current case, the Visitor API is used to transform the GBDT side of the GBDT+LR model to a regression-type boosting model. All leaf nodes are assigned new score values as extracted from the LR side.</p>

<h2 id="example-workflow">Example workflow</h2>

<p>The GBDT+LR workflow is much simpler than traditional workflows.
Specifically, there is no need to perform dedicated feature engineering work, because the GBDT+LR estimator will do it automatically and in a very thorough manner.</p>

<p>Boilerplate for assembling and fitting a GBDT+LR pipeline using user-specified <code class="language-plaintext highlighter-rouge">gbdt</code> and <code class="language-plaintext highlighter-rouge">lr</code> components:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.decoration</span> <span class="kn">import</span> <span class="n">CategoricalDomain</span><span class="p">,</span> <span class="n">ContinuousDomain</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.ensemble</span> <span class="kn">import</span> <span class="n">GBDTLRClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml.pipeline</span> <span class="kn">import</span> <span class="n">PMMLPipeline</span>

<span class="kn">import</span> <span class="nn">pandas</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(..)</span>

<span class="c1"># The names of categorical and continuous feature columns
</span><span class="n">cat_columns</span> <span class="o">=</span> <span class="p">[...]</span>
<span class="n">cont_columns</span> <span class="o">=</span> <span class="p">[...]</span>

<span class="c1"># The name of the label column
</span><span class="n">label_column</span> <span class="o">=</span> <span class="p">..</span>

<span class="k">def</span> <span class="nf">make_fit_gbdtlr</span><span class="p">(</span><span class="n">gbdt</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
  <span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">(</span>
    <span class="p">[([</span><span class="n">cat_column</span><span class="p">],</span> <span class="p">[</span><span class="n">CategoricalDomain</span><span class="p">(),</span> <span class="n">LabelBinarizer</span><span class="p">()])</span> <span class="k">for</span> <span class="n">cat_column</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span> <span class="o">+</span>
    <span class="p">[(</span><span class="n">cont_columns</span><span class="p">,</span> <span class="n">ContinuousDomain</span><span class="p">())]</span>
  <span class="p">)</span>
  <span class="n">classifier</span> <span class="o">=</span> <span class="n">GBDTLRClassifier</span><span class="p">(</span><span class="n">gbdt</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
  <span class="n">pipeline</span> <span class="o">=</span> <span class="n">PMMLPipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"mapper"</span><span class="p">,</span> <span class="n">mapper</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"classifier"</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">cat_columns</span> <span class="o">+</span> <span class="n">cont_columns</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">label_column</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">pipeline</span>
</code></pre></div></div>

<p>The most common configuration is to use <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code> as the <code class="language-plaintext highlighter-rouge">gbdt</code> component.
The “boosting” behaviour can be promoted by growing a larger number of shallower decision trees.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn2pmml</span> <span class="kn">import</span> <span class="n">sklearn2pmml</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_fit_gbdtlr</span><span class="p">(</span><span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">499</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s">"GBDT+LR.pmml"</span><span class="p">)</span>
</code></pre></div></div>

<p>Conversely, the “bagging” behaviour can be promoted by growing a smaller number of deeper decision trees.
The <code class="language-plaintext highlighter-rouge">GBDTLRClassifier</code> ensemble model accepts any PMML compatible classifier as the <code class="language-plaintext highlighter-rouge">gbdt</code> component.
For example, switching from <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code> to alternative classifier classes such as <code class="language-plaintext highlighter-rouge">ExtraTreesClassifier</code> or <code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> would reduce the risk of overfitting:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_fit_gbdtlr</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">31</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">6</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s">"RF+LR.pmml"</span><span class="p">)</span>
</code></pre></div></div>

<p>The <a href="https://github.com/dmlc/xgboost">XGBoost</a> plugin library provides the <code class="language-plaintext highlighter-rouge">xgboost.XGBClassifier</code> model, which can be used as a drop-in replacement for Scikit-Learn classifier classes:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_fit_gbdtlr</span><span class="p">(</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">299</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s">"XGB+LR.pmml"</span><span class="p">)</span>
</code></pre></div></div>

<p>The <a href="https://github.com/microsoft/LightGBM">LightGBM</a> plugin library provides the <code class="language-plaintext highlighter-rouge">lightgbm.LGBMClassifier</code> model.
One of its major selling points is proper support for categorical features.
If the training dataset contains a significant number of (high-cardinality-) categorical features, then the above <code class="language-plaintext highlighter-rouge">make_fit_gbdtlr</code> utility function should be tailored to maintain this information.</p>

<p>As discussed in <a href="/blog/2019/04/07/converting_sklearn_lightgbm_pipeline_pmml/">a recent blog post</a>, the fit method of LightGBM estimators takes an optional <code class="language-plaintext highlighter-rouge">categorical_feature</code> fit parameter.
The next challenge is about passing this parameter to a <code class="language-plaintext highlighter-rouge">LGBMClassifier</code> object, which is contained in the <code class="language-plaintext highlighter-rouge">GBDTLRClassifier</code> object, which is in turn contained in the <code class="language-plaintext highlighter-rouge">(PMML)Pipeline</code> object.
The solution follows Scikit-Learn conventions.
Namely, the fit method of the <code class="language-plaintext highlighter-rouge">GBDTLRClassifier</code> class also takes fit parameters, which are passed on to the correct component based on the prefix.</p>

<p>Boilerplate for assembling and fitting an LightGBM+LR pipeline:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="k">def</span> <span class="nf">make_fit_lgbmlr</span><span class="p">(</span><span class="n">gbdt</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
  <span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">(</span>
    <span class="p">[([</span><span class="n">cat_column</span><span class="p">],</span> <span class="p">[</span><span class="n">CategoricalDomain</span><span class="p">(),</span> <span class="n">LabelEncoder</span><span class="p">()])</span> <span class="k">for</span> <span class="n">cat_column</span> <span class="ow">in</span> <span class="n">cat_columns</span><span class="p">]</span> <span class="o">+</span>
    <span class="p">[(</span><span class="n">cont_columns</span><span class="p">,</span> <span class="n">ContinuousDomain</span><span class="p">())]</span>
  <span class="p">)</span>
  <span class="n">classifier</span> <span class="o">=</span> <span class="n">GBDTLRClassifier</span><span class="p">(</span><span class="n">gbdt</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
  <span class="n">pipeline</span> <span class="o">=</span> <span class="n">PMMLPipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"mapper"</span><span class="p">,</span> <span class="n">mapper</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"classifier"</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="c1"># The 'gbdt' component can be addressed using the `classifier__gbdt` prefix
</span>  <span class="c1"># The 'lr' component can be addressed using the `classifier__lr` prefix
</span>  <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">cat_columns</span> <span class="o">+</span> <span class="n">cont_columns</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">label_column</span><span class="p">],</span> <span class="n">classifier__gbdt__categorical_feature</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_columns</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">pipeline</span>
</code></pre></div></div>

<p>Sample usage:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_fit_lgbmlr</span><span class="p">(</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">71</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">5</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">sklearn2pmml</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s">"LGBM+LR.pmml"</span><span class="p">)</span>
</code></pre></div></div>

<p>Both XGBoost and LightGBM classifiers support missing values.
When working with sparse datasets, then it is possible to make <code class="language-plaintext highlighter-rouge">make_fit_gbdtlr</code> and <code class="language-plaintext highlighter-rouge">make_fit_lgbmlr</code> utility functions missing value-aware by replacing the default <code class="language-plaintext highlighter-rouge">LabelBinarizer</code> and <code class="language-plaintext highlighter-rouge">LabelEncoder</code> transformers with <code class="language-plaintext highlighter-rouge">sklearn2pmml.preprocessing.PMMLLabelBinarizer</code> and <code class="language-plaintext highlighter-rouge">sklearn2pmml.preprocessing.PMMLLabelEncoder</code> transformers, respectively.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li>“Audit” dataset: <a href="https://openscoring.io/resources/data/audit.csv"><code class="language-plaintext highlighter-rouge">audit.csv</code></a></li>
  <li>Python script: <a href="https://openscoring.io/resources/2019-06-19/train.py"><code class="language-plaintext highlighter-rouge">train.py</code></a></li>
</ul>

</div>



  </main>

  <footer class="footer">
  <div class="container">
    <div class="mb-6">© 2022 - Openscoring</div>

    <div class="mb-6">
      <ul class="flex gap-4 list-none">
        <li>
          <a href="mailto:info@openscoring.io" aria-label="email">
            <img src="/assets/images/email_round.svg" width="48" height="48" alt="Contact Openscoring">
          </a>
        </li>
        <li>
          <a href="https://twitter.com/openscoring" target="_blank" aria-label="twitter">
            <img src="/assets/images/twitter_round.svg" width="48" height="48" alt="Follow Openscoring on Twitter">
          </a>
        </li>
      </ul>
    </div>
  </div>
</footer>

  <script>
  var sc_project=11704106;
  var sc_security="a7d1bf16"; 
  var sc_invisible=1; 
  var sc_remove_link=1; 
</script>

<script src="https://www.statcounter.com/counter/counter.js" async></script>

<noscript>
  <div class="statcounter">
    <img class="statcounter" src="https://c.statcounter.com/11704106/0/a7d1bf16/1/" alt="Web Analytics Made Easy - Statcounter" referrerPolicy="no-referrer-when-downgrade">
  </div>
</noscript>
</body>

</html>
